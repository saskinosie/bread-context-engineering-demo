{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 2: Setting Up Prompt Baking with Bread AI\n",
    "\n",
    "This notebook demonstrates how to use Bread AI to \"bake\" a system prompt into model weights, eliminating the need for runtime system prompts.\n",
    "\n",
    "**Process:**\n",
    "1. Load the system prompt (expertise definition)\n",
    "2. Configure the baking process\n",
    "3. Generate synthetic training data (stim)\n",
    "4. Run rollout with prompted model\n",
    "5. Bake the behavior into weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Prompt Baking?\n",
    "\n",
    "Prompt baking encodes prompt behavior directly into model weights. After baking, the model exhibits the prompted behavior with **ZERO input tokens** required at inference time.\n",
    "\n",
    "Think of it like:\n",
    "- **Traditional**: Giving someone instructions every single time\n",
    "- **Baked**: Training someone once, then they just know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "sys.path.append(str(Path(\".\").resolve().parent))\n",
    "\n",
    "from utils.helpers import load_system_prompt, count_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded system prompt: 551 tokens\n",
      "Loaded bake configuration\n",
      "  Repository: rag-expert-model\n",
      "  Base model: gpt-4\n",
      "  Method: weight-space-editing\n"
     ]
    }
   ],
   "source": [
    "# Load the expert system prompt\n",
    "system_prompt = load_system_prompt()\n",
    "print(f\"Loaded system prompt: {count_tokens(system_prompt)} tokens\")\n",
    "\n",
    "# Load bake configuration\n",
    "with open(\"../configs/bake_config.yaml\", 'r') as f:\n",
    "    bake_config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Loaded bake configuration\")\n",
    "print(f\"  Repository: {bake_config['repository']['name']}\")\n",
    "print(f\"  Base model: {bake_config['bake']['base_model']}\")\n",
    "print(f\"  Method: {bake_config['bake']['method']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock Bread AI Client\n",
    "\n",
    "In production, use `from bread import BreadClient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bread AI client initialized\n"
     ]
    }
   ],
   "source": [
    "class MockBreadClient:\n",
    "    \"\"\"Mock Bread AI client for demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.repo = None\n",
    "    \n",
    "    def create_repository(self, name: str, description: str):\n",
    "        print(f\"   Created repository: {name}\")\n",
    "        self.repo = {\"name\": name, \"description\": description}\n",
    "        return self.repo\n",
    "    \n",
    "    def create_prompt(self, repo_name: str, prompt_text: str, metadata: dict):\n",
    "        print(f\"   Registered prompt: {metadata.get('name', 'unnamed')}\")\n",
    "        print(f\"   Tokens: ~{count_tokens(prompt_text)}\")\n",
    "        return {\"id\": \"prompt_123\", \"tokens\": count_tokens(prompt_text)}\n",
    "    \n",
    "    def create_stim_job(self, repo_name: str, config: dict):\n",
    "        print(f\"   Started stim generation job\")\n",
    "        print(f\"   Target: {config.get('num_samples', 100)} samples\")\n",
    "        return {\"job_id\": \"stim_456\", \"status\": \"running\"}\n",
    "    \n",
    "    def create_rollout_job(self, repo_name: str, stim_job_id: str, config: dict):\n",
    "        print(f\"   Started rollout job\")\n",
    "        return {\"job_id\": \"rollout_789\", \"status\": \"running\"}\n",
    "    \n",
    "    def create_bake_job(self, repo_name: str, rollout_job_id: str, config: dict):\n",
    "        print(f\"   Started baking job\")\n",
    "        print(f\"   Base model: {config.get('base_model', 'gpt-4')}\")\n",
    "        print(f\"   Method: {config.get('method', 'weight-space-editing')}\")\n",
    "        return {\"job_id\": \"bake_012\", \"status\": \"running\"}\n",
    "    \n",
    "    def get_baked_model(self, repo_name: str, version: str = \"latest\"):\n",
    "        return {\n",
    "            \"model_id\": \"rag-expert-baked-v1\",\n",
    "            \"repo\": repo_name,\n",
    "            \"version\": version,\n",
    "            \"status\": \"ready\"\n",
    "        }\n",
    "\n",
    "# Initialize client\n",
    "client = MockBreadClient(api_key=\"demo_key\")\n",
    "print(\"Bread AI client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Model Repository\n",
    "\n",
    "Bread AI uses git-like version control for models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Created repository: rag-expert-model\n"
     ]
    }
   ],
   "source": [
    "repo_config = bake_config[\"repository\"]\n",
    "\n",
    "repo = client.create_repository(\n",
    "    name=repo_config[\"name\"],\n",
    "    description=repo_config[\"description\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Register System Prompt for Baking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Registered prompt: expert_system_prompt\n",
      "   Tokens: ~551\n"
     ]
    }
   ],
   "source": [
    "prompt_config = bake_config[\"prompts\"][0]\n",
    "\n",
    "prompt = client.create_prompt(\n",
    "    repo_name=repo[\"name\"],\n",
    "    prompt_text=system_prompt,\n",
    "    metadata={\"name\": prompt_config[\"name\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Synthetic Training Data (Stim)\n",
    "\n",
    "**Stim** = Diverse user queries that test the expert behavior. The model will be tested on these queries during rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stim configuration:\n",
      "  Model: gpt-4\n",
      "  Samples: 100\n",
      "  Topics: vector database architecture questions, RAG system design problems, embedding model selection...\n",
      "   Started stim generation job\n",
      "   Target: 100 samples\n",
      "\n",
      "Generating synthetic data...\n",
      "Generated 100 diverse queries\n"
     ]
    }
   ],
   "source": [
    "stim_config = bake_config[\"generators\"][\"stim\"]\n",
    "\n",
    "print(\"Stim configuration:\")\n",
    "print(f\"  Model: {stim_config['model']}\")\n",
    "print(f\"  Samples: {stim_config['num_samples']}\")\n",
    "print(f\"  Topics: {', '.join(stim_config['topics'][:3])}...\")\n",
    "\n",
    "stim_job = client.create_stim_job(\n",
    "    repo_name=repo[\"name\"],\n",
    "    config=stim_config\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerating synthetic data...\")\n",
    "print(f\"Generated {stim_config['num_samples']} diverse queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Rollout (Prompted Model Responses)\n",
    "\n",
    "**Rollout** = Run the model WITH system prompt on stim data. This captures how the expert model should behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollout configuration:\n",
      "  Model: gpt-4\n",
      "  Use system prompt: True\n",
      "   Started rollout job\n",
      "\n",
      "Collecting expert responses...\n",
      "Collected 100 expert responses\n"
     ]
    }
   ],
   "source": [
    "rollout_config = bake_config[\"generators\"][\"rollout\"]\n",
    "\n",
    "print(\"Rollout configuration:\")\n",
    "print(f\"  Model: {rollout_config['model']}\")\n",
    "print(f\"  Use system prompt: {rollout_config['use_system_prompt']}\")\n",
    "\n",
    "rollout_job = client.create_rollout_job(\n",
    "    repo_name=repo[\"name\"],\n",
    "    stim_job_id=stim_job[\"job_id\"],\n",
    "    config=rollout_config\n",
    ")\n",
    "\n",
    "print(f\"\\nCollecting expert responses...\")\n",
    "print(f\"Collected {stim_config['num_samples']} expert responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Bake Behavior into Model Weights\n",
    "\n",
    "**Baking** = Surgically edit model weights to internalize behavior. This uses advanced model editing techniques (not traditional fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bake configuration:\n",
      "  Base model: gpt-4\n",
      "  Method: weight-space-editing\n",
      "  Preserve general capabilities: True\n",
      "   Started baking job\n",
      "   Base model: gpt-4\n",
      "   Method: weight-space-editing\n",
      "\n",
      "Applying weight-space edits...\n",
      "Preserving general capabilities...\n",
      "Validating behavior preservation...\n",
      "Baking complete!\n"
     ]
    }
   ],
   "source": [
    "bake_job_config = bake_config[\"bake\"]\n",
    "\n",
    "print(\"Bake configuration:\")\n",
    "print(f\"  Base model: {bake_job_config['base_model']}\")\n",
    "print(f\"  Method: {bake_job_config['method']}\")\n",
    "print(f\"  Preserve general capabilities: {bake_job_config['constraints']['preserve_general_capabilities']}\")\n",
    "\n",
    "bake_job = client.create_bake_job(\n",
    "    repo_name=repo[\"name\"],\n",
    "    rollout_job_id=rollout_job[\"job_id\"],\n",
    "    config=bake_job_config\n",
    ")\n",
    "\n",
    "print(f\"\\nApplying weight-space edits...\")\n",
    "print(f\"Preserving general capabilities...\")\n",
    "print(f\"Validating behavior preservation...\")\n",
    "print(f\"Baking complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Retrieve Baked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baked model ready!\n",
      "  Model ID: rag-expert-baked-v1\n",
      "  Status: ready\n",
      "  Version: latest\n"
     ]
    }
   ],
   "source": [
    "baked_model = client.get_baked_model(repo_name=repo[\"name\"])\n",
    "\n",
    "print(f\"Baked model ready!\")\n",
    "print(f\"  Model ID: {baked_model['model_id']}\")\n",
    "print(f\"  Status: {baked_model['status']}\")\n",
    "print(f\"  Version: {baked_model['version']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What Just Happened\n",
    "\n",
    "1. Loaded ~550-token expert system prompt\n",
    "2. Generated 100 diverse test queries\n",
    "3. Collected expert responses WITH system prompt\n",
    "4. Surgically edited model weights to internalize behavior\n",
    "5. Created new model with expertise \"baked in\"\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "- Expert behavior is now **IN THE WEIGHTS**\n",
    "- **Zero** system prompt tokens needed at inference\n",
    "- **Faster** response times (no prompt processing)\n",
    "- **Consistent** behavior guaranteed\n",
    "- **Version-controlled** expertise (git-like workflow)\n",
    "\n",
    "## Next Step\n",
    "\n",
    "Run **Demo 3** to use the baked model and compare results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
