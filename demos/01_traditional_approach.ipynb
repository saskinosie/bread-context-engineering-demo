{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: Traditional Approach with System Prompts\n",
    "\n",
    "This notebook demonstrates the traditional approach to context engineering where we send a lengthy system prompt with **every** API request.\n",
    "\n",
    "**Key Points:**\n",
    "- System prompt sent with EVERY request\n",
    "- High token overhead\n",
    "- Consistent but expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append(str(Path(\".\").resolve().parent))\n",
    "\n",
    "from utils.helpers import (\n",
    "    load_system_prompt,\n",
    "    count_tokens,\n",
    "    get_sample_queries,\n",
    "    format_comparison,\n",
    "    calculate_cost\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Expert System Prompt\n",
    "\n",
    "This is a 551-token system prompt that defines RAG/Vector DB expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt tokens: ~551\n",
      "\n",
      "--- System Prompt Preview ---\n",
      "\n",
      "You are an expert AI Solutions Architect with over 10 years of experience specializing in vector databases, retrieval-augmented generation (RAG) systems, and production AI deployments.\n",
      "\n",
      "Your core expertise includes:\n",
      "\n",
      "**Vector Databases & Search:**\n",
      "- Deep knowledge of Weaviate, Pinecone, Qdrant, and Milvus architectures\n",
      "- Expertise in embedding models (OpenAI, Cohere, sentence-transformers)\n",
      "- Advanced understanding of ANN algorithms (HNSW, IVF, Product Quantization)\n",
      "- Hybrid search strategies com...\n"
     ]
    }
   ],
   "source": [
    "system_prompt = load_system_prompt()\n",
    "system_token_count = count_tokens(system_prompt)\n",
    "\n",
    "print(f\"System prompt tokens: ~{system_token_count}\")\n",
    "print(f\"\\n--- System Prompt Preview ---\\n\")\n",
    "print(system_prompt[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock OpenAI Client\n",
    "\n",
    "For demonstration purposes. In production, use the actual OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client initialized\n"
     ]
    }
   ],
   "source": [
    "class MockOpenAIClient:\n",
    "    \"\"\"Mock client for demonstration purposes\"\"\"\n",
    "    def chat_completions_create(self, messages, model=\"gpt-4\", **kwargs):\n",
    "        user_query = messages[-1][\"content\"]\n",
    "        return MockResponse(\n",
    "            f\"Based on my expertise in RAG systems and vector databases, \"\n",
    "            f\"here's how I would approach '{user_query[:50]}...': \"\n",
    "            f\"[Detailed expert response simulated here.]\"\n",
    "        )\n",
    "\n",
    "class MockResponse:\n",
    "    def __init__(self, content):\n",
    "        self.choices = [type('obj', (object,), {'message': type('obj', (object,), {'content': content})()})()]\n",
    "\n",
    "client = MockOpenAIClient()\n",
    "print(\"Client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Query Function\n",
    "\n",
    "Notice how we include the full system prompt with **every single request**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_query(client, system_prompt: str, user_query: str):\n",
    "    \"\"\"\n",
    "    Execute a query using the traditional approach with system prompt\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},  # <-- Sent EVERY time!\n",
    "        {\"role\": \"user\", \"content\": user_query}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat_completions_create(messages=messages, model=\"gpt-4\")\n",
    "    system_tokens = count_tokens(system_prompt)\n",
    "    \n",
    "    return response.choices[0].message.content, system_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Sample Queries\n",
    "\n",
    "Let's run a few queries and see the token overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 3 sample queries...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query 1: What's the best chunking strategy for long technical documen...\n",
      "System tokens: 551\n",
      "Response: Based on my expertise in RAG systems and vector databases, here's how I would approach 'What's the b...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query 2: How do I optimize Weaviate for 100M+ vectors?...\n",
      "System tokens: 551\n",
      "Response: Based on my expertise in RAG systems and vector databases, here's how I would approach 'How do I opt...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query 3: Should I use hybrid search or pure vector search for my use ...\n",
      "System tokens: 551\n",
      "Response: Based on my expertise in RAG systems and vector databases, here's how I would approach 'Should I use...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Total system prompt tokens across 3 queries: 1653\n"
     ]
    }
   ],
   "source": [
    "queries = get_sample_queries()[:3]\n",
    "\n",
    "print(f\"Running {len(queries)} sample queries...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_system_tokens = 0\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    response, system_tokens = traditional_query(client, system_prompt, query)\n",
    "    total_system_tokens += system_tokens\n",
    "    \n",
    "    print(f\"\\nQuery {i}: {query[:60]}...\")\n",
    "    print(f\"System tokens: {system_tokens}\")\n",
    "    print(f\"Response: {response[:100]}...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nTotal system prompt tokens across {len(queries)} queries: {total_system_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Token Overhead at Scale\n",
    "\n",
    "Let's visualize what this looks like at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN BREAKDOWN PER REQUEST\n",
      "==================================================\n",
      "System Prompt:     551 tokens\n",
      "User Query:         50 tokens (avg)\n",
      "--------------------------------------------------\n",
      "Total:             601 tokens\n",
      "\n",
      "System prompt is 91.7% of every request!\n"
     ]
    }
   ],
   "source": [
    "avg_user_tokens = 50\n",
    "total_tokens_per_request = system_token_count + avg_user_tokens\n",
    "\n",
    "print(\"TOKEN BREAKDOWN PER REQUEST\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"System Prompt:  {system_token_count:>6} tokens\")\n",
    "print(f\"User Query:     {avg_user_tokens:>6} tokens (avg)\")\n",
    "print(f\"-\" * 50)\n",
    "print(f\"Total:          {total_tokens_per_request:>6} tokens\")\n",
    "print(f\"\\nSystem prompt is {(system_token_count/total_tokens_per_request)*100:.1f}% of every request!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Analysis at Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONTHLY COST ANALYSIS (Traditional Approach)\n",
      "============================================================\n",
      "Scenario                  Monthly Requests    Monthly Cost\n",
      "------------------------------------------------------------\n",
      "10K requests/day                  300,000 $      5,409.00\n",
      "100K requests/day               3,000,000 $     54,090.00\n",
      "1M requests/day                30,000,000 $    540,900.00\n",
      "\n",
      "These costs are JUST for input tokens (system + user prompt)!\n"
     ]
    }
   ],
   "source": [
    "scenarios = [\n",
    "    (\"10K requests/day\", 10_000 * 30),\n",
    "    (\"100K requests/day\", 100_000 * 30),\n",
    "    (\"1M requests/day\", 1_000_000 * 30),\n",
    "]\n",
    "\n",
    "print(\"MONTHLY COST ANALYSIS (Traditional Approach)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Scenario':<25} {'Monthly Requests':>15} {'Monthly Cost':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, monthly_requests in scenarios:\n",
    "    total_tokens = total_tokens_per_request * monthly_requests\n",
    "    cost = calculate_cost(total_tokens)\n",
    "    print(f\"{name:<25} {monthly_requests:>15,} ${cost:>14,.2f}\")\n",
    "\n",
    "print(\"\\nThese costs are JUST for input tokens (system + user prompt)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Preview\n",
    "\n",
    "What if we could eliminate that system prompt overhead entirely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRADITIONAL vs BAKED MODEL COMPARISON (1M requests)\n",
      "======================================================================\n",
      "Metric                                  Traditional           Baked\n",
      "----------------------------------------------------------------------\n",
      "Tokens per request                              601              50\n",
      "Monthly cost (1M req)               $     18,030.00 $      1,500.00\n",
      "----------------------------------------------------------------------\n",
      "Token savings                                 91.7%\n",
      "Monthly savings                     $     16,530.00\n",
      "Annual savings                      $    198,360.00\n"
     ]
    }
   ],
   "source": [
    "metrics = format_comparison(\n",
    "    traditional_tokens=total_tokens_per_request,\n",
    "    baked_tokens=avg_user_tokens,  # Just the user query!\n",
    "    num_requests=1_000_000\n",
    ")\n",
    "\n",
    "print(\"TRADITIONAL vs BAKED MODEL COMPARISON (1M requests)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<35} {'Traditional':>15} {'Baked':>15}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Tokens per request':<35} {metrics['traditional']['tokens_per_request']:>15} {metrics['baked']['tokens_per_request']:>15}\")\n",
    "print(f\"{'Monthly cost (1M req)':<35} ${metrics['traditional']['cost']:>14,.2f} ${metrics['baked']['cost']:>14,.2f}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Token savings':<35} {metrics['savings']['tokens_percent']:>14.1f}%\")\n",
    "print(f\"{'Monthly savings':<35} ${metrics['savings']['total_cost']:>14,.2f}\")\n",
    "print(f\"{'Annual savings':<35} ${metrics['savings']['annual_cost_1m_requests']:>14,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "- System prompt adds **~550 tokens** to EVERY request\n",
    "- At scale, this becomes a **significant cost driver**\n",
    "- Longer prompts = **higher latency**\n",
    "- Must maintain consistency across all deployments\n",
    "\n",
    "## Next Step\n",
    "\n",
    "Run **Demo 2** to see how Bread AI eliminates this overhead through \"prompt baking\"!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
