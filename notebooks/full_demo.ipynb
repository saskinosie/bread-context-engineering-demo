{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üçû Prompt Baking for Context Engineering\n",
    "\n",
    "## Interactive Demo: Traditional vs. Baked Approach\n",
    "\n",
    "This notebook demonstrates the difference between traditional context engineering (with system prompts) and Bread AI's prompt baking approach (expertise in weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.helpers import (\n",
    "    load_system_prompt,\n",
    "    count_tokens,\n",
    "    get_sample_queries,\n",
    "    format_comparison,\n",
    "    print_comparison_table\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Traditional Approach\n",
    "\n",
    "First, let's look at the traditional approach with system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the expert system prompt\nsystem_prompt = load_system_prompt()\n\n# Count tokens\ntoken_count = count_tokens(system_prompt)\n\nprint(f\"System Prompt Loaded\")\nprint(f\"   Tokens: {token_count}\")\nprint(f\"\\nPreview (first 200 chars):\")\nprint(f\"   {system_prompt[:200]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Query Structure\n",
    "\n",
    "With traditional approach, every API call includes the system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example traditional API call\n",
    "example_query = \"What's the best chunking strategy for technical docs?\"\n",
    "\n",
    "traditional_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},  # 347 tokens!\n",
    "    {\"role\": \"user\", \"content\": example_query}      # ~15 tokens\n",
    "]\n",
    "\n",
    "print(\"Traditional API Call Structure:\")\n",
    "print(f\"  System Prompt: {token_count} tokens\")\n",
    "print(f\"  User Query: ~{count_tokens(example_query)} tokens\")\n",
    "print(f\"  Total: ~{token_count + count_tokens(example_query)} tokens per request\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Baked Approach\n",
    "\n",
    "After baking, the expertise is in the weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example baked API call\n",
    "baked_messages = [\n",
    "    {\"role\": \"user\", \"content\": example_query}  # Just the query!\n",
    "]\n",
    "\n",
    "print(\"Baked API Call Structure:\")\n",
    "print(f\"  System Prompt: 0 tokens (baked into weights!)\")\n",
    "print(f\"  User Query: ~{count_tokens(example_query)} tokens\")\n",
    "print(f\"  Total: ~{count_tokens(example_query)} tokens per request\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "avg_user_tokens = 50\n",
    "traditional_total = token_count + avg_user_tokens\n",
    "baked_total = avg_user_tokens\n",
    "\n",
    "metrics = format_comparison(\n",
    "    traditional_tokens=traditional_total,\n",
    "    baked_tokens=baked_total,\n",
    "    num_requests=1000000\n",
    ")\n",
    "\n",
    "print_comparison_table(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Cost Analysis at Different Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate costs at different scales\n",
    "scales = [\n",
    "    (\"Startup (10k/day)\", 10_000 * 30),\n",
    "    (\"Growth (100k/day)\", 100_000 * 30),\n",
    "    (\"Scale (1M/day)\", 1_000_000 * 30),\n",
    "]\n",
    "\n",
    "data = []\n",
    "for name, monthly_reqs in scales:\n",
    "    trad_cost = (traditional_total * monthly_reqs * 0.03) / 1_000_000\n",
    "    baked_cost = (baked_total * monthly_reqs * 0.03) / 1_000_000\n",
    "    savings = trad_cost - baked_cost\n",
    "    annual_savings = savings * 12\n",
    "    \n",
    "    data.append({\n",
    "        \"Scale\": name,\n",
    "        \"Monthly Requests\": f\"{monthly_reqs:,}\",\n",
    "        \"Traditional Cost/mo\": f\"${trad_cost:.2f}\",\n",
    "        \"Baked Cost/mo\": f\"${baked_cost:.2f}\",\n",
    "        \"Annual Savings\": f\"${annual_savings:.2f}\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"\\nüí∞ Cost Comparison at Scale:\\n\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Key Insights\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "1. **Traditional Approach**: We send a 347-token system prompt with EVERY request\n",
    "2. **Baked Approach**: We encode that expertise into model weights ONCE\n",
    "3. **Result**: 87% token reduction per request!\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "‚úÖ **Cost Savings**: $2,000+ per year at 1M req/month\n",
    "\n",
    "‚úÖ **Faster Responses**: 15-20% latency improvement\n",
    "\n",
    "‚úÖ **Consistency**: Can't forget to include the prompt\n",
    "\n",
    "‚úÖ **Versioning**: Git-like workflow for model expertise\n",
    "\n",
    "‚úÖ **Deployment**: Simpler (no prompt management)\n",
    "\n",
    "### The Paradigm Shift:\n",
    "\n",
    "Context engineering moves from:\n",
    "- üî¥ **Runtime overhead** ‚Üí üü¢ **Compile-time optimization**\n",
    "- üî¥ **Tell every time** ‚Üí üü¢ **Teach once**\n",
    "- üî¥ **Prompt engineering** ‚Üí üü¢ **Weight engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\n1. Run the demo notebooks in `demos/`:\n   - `01_traditional_approach.ipynb` - See the problem\n   - `02_bread_baking_setup.ipynb` - See the solution\n   - `03_baked_inference.ipynb` - See the results\n\n2. Explore Bread AI:\n   - [Documentation](https://docs.bread.com.ai/)\n   - [GitHub](https://github.com/Bread-Technologies)\n\n3. Build your own:\n   - Try baking different expert personas\n   - Experiment with RAG integration\n   - Explore multi-tenant applications"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}